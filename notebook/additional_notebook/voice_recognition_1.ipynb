{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import os \n",
    "\n",
    "  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir=os.listdir(\"E:/speech_recognition/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/speech_recognition/data/audio_1.wav\n",
      "E:/speech_recognition/data/audio_2.wav\n",
      "E:/speech_recognition/data/audio_3.wav\n"
     ]
    }
   ],
   "source": [
    "mfcc=[]\n",
    "for i in root_dir:\n",
    "    audio_paths=f'E:/speech_recognition/data/{i}'\n",
    "    print(audio_paths)\n",
    "    \n",
    "    # load audio file and slice it to ensure length consistency among different files\n",
    "    signal,sample_rate = librosa.load(audio_paths)\n",
    "    \n",
    "    # extract MFCCs\n",
    "    MFCCs = librosa.feature.mfcc(signal, sample_rate)\n",
    "    mfcc.append( MFCCs)                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 171)\n"
     ]
    }
   ],
   "source": [
    "print(mfcc[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mfcc_feature(vis_mfcc_feature):\n",
    "    # plot the MFCC feature\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(vis_mfcc_feature, cmap=plt.cm.jet, aspect='auto')\n",
    "    plt.title('Normalized MFCC')\n",
    "    plt.ylabel('Time')\n",
    "    plt.xlabel('MFCC Coefficient')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    ax.set_xticks(np.arange(0, 13, 2), minor=False);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_mfcc_feature(mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels=[\"আমি এসএসএল ওয়ারলেসে জব\",\n",
    "        #\"আমি ডাটা টিমের সদস্য\",\n",
    "        #\"আমাদের ডেটা টিমে দুইজন জমজ ভাই আছে\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels():\n",
    "    labels =[\"আমি এসএসএল ওয়ারলেসে জব\",\n",
    "            \"আমি ডাটা টিমের সদস্য\",\n",
    "            \"আমাদের ডেটা টিমে দুইজন জমজ ভাই আছে\"] \n",
    "    #labels=os.listdir(path)\n",
    "    print(len(labels))\n",
    "    label_indices = np.arange(0, len(labels))\n",
    "    print(label_indices)\n",
    "    return labels, label_indices, to_categorical(label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[0 1 2]\n",
      "['আমি এসএসএল ওয়ারলেসে জব', 'আমি ডাটা টিমের সদস্য', 'আমাদের ডেটা টিমে দুইজন জমজ ভাই আছে']\n"
     ]
    }
   ],
   "source": [
    "#data_path=\"E:/speech_recognition/data/\"\n",
    "labels,label_indices,_=get_labels()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel-frequency cepstral coefficients\n",
    "def wav2mfcc(file_path,max_len, n_mfcc):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    wave = np.asfortranarray(wave[::3])\n",
    "    mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=n_mfcc)\n",
    "\n",
    "    # If maximum length exceeds mfcc lengths then pad the remaining ones\n",
    "    if (max_len > mfcc.shape[1]):\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    # Else cutoff the remaining parts\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    \n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_data_to_array(path, max_len, n_mfcc):\n",
    "    #labels,_, _ = get_labels()\n",
    "\n",
    "    #for label in labels:\n",
    "        # Init mfcc vectors\n",
    "    mfcc_vectors = []\n",
    "   \n",
    "    for wav in path:\n",
    "        wavfile=f'E:/speech_recognition/data/{wav}'\n",
    "        print(wavfile)\n",
    "        mfcc = wav2mfcc(wavfile, max_len, n_mfcc)\n",
    "        mfcc_vectors.append(mfcc)\n",
    "    np.save('E:/speech_recognition/wav', mfcc_vectors)\n",
    "    return mfcc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/speech_recognition/data/audio_1.wav\n",
      "E:/speech_recognition/data/audio_2.wav\n",
      "E:/speech_recognition/data/audio_3.wav\n",
      "(20, 200)\n"
     ]
    }
   ],
   "source": [
    "mfcc_vec=save_data_to_array(root_dir,200,20)\n",
    "print(mfcc_vec[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "x= np.load('E:/speech_recognition/wav.npy')\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting first arrays\n",
    "#X = np.load(labels[0] + '.npy')\n",
    "#y = np.zeros(X.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(split_ratio=0.6, random_state=42):\n",
    "    # Get available labels\n",
    "    labels, indices, _ = get_labels()\n",
    "\n",
    "    # Getting first arrays\n",
    "    X = np.load('E:/speech_recognition/wav.npy')\n",
    "    y = np.zeros(X.shape[0])\n",
    "    #print(y)\n",
    "    # Append all of the dataset into one single array, same goes for y\n",
    "    for i, label in enumerate(labels[1:]):\n",
    "        x = np.load('E:/speech_recognition/wav.npy')\n",
    "        X = np.vstack((X, x))\n",
    "        y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
    "        print(y)\n",
    "    assert X.shape[0] == len(y)\n",
    "\n",
    "    return train_test_split(X, y, test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_train_test(split_ratio=0.6, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(x,label_indices,test_size=0.1,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 20, 200)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature dimension\n",
    "channels = 1\n",
    "max_len = 200\n",
    "buckets = 20\n",
    "epochs = 48\n",
    "batch_size = 100\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "#X_train = X_train.reshape(X_train.shape[0],buckets, max_len, channels)\n",
    "#X_test = X_test.reshape(X_test.shape[0],buckets,max_len, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 20, 200) (1, 20, 200)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape)\n",
    "#plt.imshow(X_train[:, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2] [0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) (1,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]] [[1.]]\n"
     ]
    }
   ],
   "source": [
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)\n",
    "print(y_train_hot,y_test_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from preprocess import *\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, LSTM\n",
    "from keras.utils import to_categorical\n",
    "#import wandb\n",
    "#from wandb.keras import WandbCallback\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=3\n",
    "\n",
    "#build a simple cnn model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(buckets,max_len)))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 12003     \n",
      "=================================================================\n",
      "Total params: 12,003\n",
      "Trainable params: 12,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0])\n",
    "X_train = X_train.reshape(X_train.shape[0],buckets, max_len)\n",
    "X_test = X_test.reshape(X_test.shape[0],buckets,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 20, 200)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 200)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2 samples, validate on 1 samples\n",
      "Epoch 1/48\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 122.4453 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/48\n",
      "2/2 [==============================] - 0s 998us/step - loss: 10.0779 - accuracy: 0.0000e+00 - val_loss: 8.1836 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 48.7357 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/48\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 81.9535 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/48\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 110.0301 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/48\n",
      "2/2 [==============================] - 0s 998us/step - loss: 2.9802e-07 - accuracy: 1.0000 - val_loss: 134.2287 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/48\n",
      "2/2 [==============================] - 0s 997us/step - loss: 1.5715e-04 - accuracy: 1.0000 - val_loss: 155.3666 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 175.0989 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 1.0000 - val_loss: 194.8384 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/48\n",
      "2/2 [==============================] - 0s 997us/step - loss: 1.2372e-04 - accuracy: 1.0000 - val_loss: 212.4595 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/48\n",
      "2/2 [==============================] - 0s 997us/step - loss: 3.5763e-07 - accuracy: 1.0000 - val_loss: 228.2513 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 242.4503 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 255.2506 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/48\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 266.8147 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/48\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 277.2800 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 286.7642 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/48\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 295.3692 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 303.1837 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/48\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 310.2859 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/48\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 316.7448 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 322.6215 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/48\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 327.9705 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/48\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 332.8409 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 337.2766 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/48\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 341.3170 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/48\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 344.9977 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/48\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 348.3513 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/48\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 351.4067 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 354.1908 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/48\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 356.7273 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 359.0380 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 361.1434 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/48\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 363.0612 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 364.8080 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/48\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 366.3990 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/48\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 367.8478 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/48\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 369.1672 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 370.3681 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/48\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 371.4615 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 372.4566 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/48\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 373.3623 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/48\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 374.1862 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 374.9359 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/48\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 375.6178 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 376.2379 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 376.8019 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 377.3149 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/48\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 377.7810 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x276eb4f1b88>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=epochs, \n",
    "          validation_data=(X_test,y_test),\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 200)\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "import cv2\n",
    "from numpy import zeros, newaxis\n",
    "#print(mfcc_vec[1].shape)\n",
    "\n",
    "\n",
    "#.........take a random voice..............\n",
    "mfcc_1=mfcc_vec[1][newaxis,:, :,]\n",
    "print(mfcc_1.shape)\n",
    "predict = model.predict_classes(mfcc_1)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "আমি ডাটা টিমের সদস্য\n"
     ]
    }
   ],
   "source": [
    "if predict==[0]:\n",
    "    print(\"আমি এসএসএল ওয়ারলেসে জব\")\n",
    "elif predict==[1]:\n",
    "    print(\"আমি ডাটা টিমের সদস্য\")\n",
    "else:\n",
    "    print(\"আমাদের ডেটা টিমে দুইজন জমজ ভাই আছে\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "x=[ 5,40, 51, 81, 12, 46 ,12]\n",
    "print(len(x[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
